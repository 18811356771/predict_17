{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GZ_Traffic_Tianchi_Session_1\n",
    "- ZS167275\n",
    "- 线上0.300成绩，最终第一赛季53名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/home/chenzh/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from scipy.stats import mode\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 链路信息，包括长宽、链路等级\n",
    "link_info = pd.read_table('data/gy_contest_link_info.txt',sep=';')\n",
    "link_info = link_info.sort_values('link_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               link_ID        date                              time_interval  \\\n",
      "0  4377906283422600514  2017-05-06  [2017-05-06 11:04:00,2017-05-06 11:06:00)   \n",
      "1  3377906289434510514  2017-05-06  [2017-05-06 10:42:00,2017-05-06 10:44:00)   \n",
      "2  3377906285934510514  2017-05-06  [2017-05-06 11:56:00,2017-05-06 11:58:00)   \n",
      "3  3377906285934510514  2017-05-06  [2017-05-06 17:46:00,2017-05-06 17:48:00)   \n",
      "4  3377906287934510514  2017-05-06  [2017-05-06 10:52:00,2017-05-06 10:54:00)   \n",
      "\n",
      "   travel_time  \n",
      "0          3.0  \n",
      "1          1.0  \n",
      "2         35.2  \n",
      "3         26.2  \n",
      "4         10.4  \n",
      "(7705175, 4)\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_table(u'data/gy_contest_traveltime_training_data_second.txt',sep=';')\n",
    "training_data.columns = ['link_ID', 'date', 'time_interval', 'travel_time']\n",
    "print training_data.head()\n",
    "print training_data.shape\n",
    "training_data = pd.merge(training_data,link_info,on='link_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118800, 7)\n"
     ]
    }
   ],
   "source": [
    "# 测试数据\n",
    "testing_data = pd.read_table(u'data/gy_contest_result_template.txt',sep='#',header=None)\n",
    "testing_data.columns = ['link_ID', 'date', 'time_interval', 'travel_time']\n",
    "testing_data = pd.merge(testing_data,link_info,on='link_ID')\n",
    "testing_data['travel_time'] = np.NaN\n",
    "print testing_data.shape\n",
    "# 所有数据\n",
    "feature_date = pd.concat([training_data,testing_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     link_ID        date  \\\n",
      "4316799  3377906280028510514  2017-03-01   \n",
      "4317063  3377906280028510514  2017-03-01   \n",
      "4316809  3377906280028510514  2017-03-01   \n",
      "4316850  3377906280028510514  2017-03-01   \n",
      "4316947  3377906280028510514  2017-03-01   \n",
      "4316810  3377906280028510514  2017-03-01   \n",
      "4316765  3377906280028510514  2017-03-01   \n",
      "4317064  3377906280028510514  2017-03-01   \n",
      "4317049  3377906280028510514  2017-03-01   \n",
      "4316885  3377906280028510514  2017-03-01   \n",
      "4317065  3377906280028510514  2017-03-01   \n",
      "4317039  3377906280028510514  2017-03-01   \n",
      "4316948  3377906280028510514  2017-03-01   \n",
      "4316811  3377906280028510514  2017-03-01   \n",
      "4317066  3377906280028510514  2017-03-01   \n",
      "4316989  3377906280028510514  2017-03-01   \n",
      "4316932  3377906280028510514  2017-03-01   \n",
      "4316916  3377906280028510514  2017-03-01   \n",
      "4316766  3377906280028510514  2017-03-01   \n",
      "4316767  3377906280028510514  2017-03-01   \n",
      "4317067  3377906280028510514  2017-03-01   \n",
      "4316917  3377906280028510514  2017-03-01   \n",
      "4316960  3377906280028510514  2017-03-01   \n",
      "4316886  3377906280028510514  2017-03-01   \n",
      "4316949  3377906280028510514  2017-03-01   \n",
      "4316961  3377906280028510514  2017-03-01   \n",
      "4316950  3377906280028510514  2017-03-01   \n",
      "4317017  3377906280028510514  2017-03-01   \n",
      "4316918  3377906280028510514  2017-03-01   \n",
      "4316990  3377906280028510514  2017-03-01   \n",
      "...                      ...         ...   \n",
      "79170    9377906289175510514  2017-06-30   \n",
      "79171    9377906289175510514  2017-06-30   \n",
      "79172    9377906289175510514  2017-06-30   \n",
      "79173    9377906289175510514  2017-06-30   \n",
      "79174    9377906289175510514  2017-06-30   \n",
      "79175    9377906289175510514  2017-06-30   \n",
      "79176    9377906289175510514  2017-06-30   \n",
      "79177    9377906289175510514  2017-06-30   \n",
      "79178    9377906289175510514  2017-06-30   \n",
      "79179    9377906289175510514  2017-06-30   \n",
      "79180    9377906289175510514  2017-06-30   \n",
      "79181    9377906289175510514  2017-06-30   \n",
      "79182    9377906289175510514  2017-06-30   \n",
      "79183    9377906289175510514  2017-06-30   \n",
      "79184    9377906289175510514  2017-06-30   \n",
      "79185    9377906289175510514  2017-06-30   \n",
      "79186    9377906289175510514  2017-06-30   \n",
      "79187    9377906289175510514  2017-06-30   \n",
      "79188    9377906289175510514  2017-06-30   \n",
      "79189    9377906289175510514  2017-06-30   \n",
      "79190    9377906289175510514  2017-06-30   \n",
      "79191    9377906289175510514  2017-06-30   \n",
      "79192    9377906289175510514  2017-06-30   \n",
      "79193    9377906289175510514  2017-06-30   \n",
      "79194    9377906289175510514  2017-06-30   \n",
      "79195    9377906289175510514  2017-06-30   \n",
      "79196    9377906289175510514  2017-06-30   \n",
      "79197    9377906289175510514  2017-06-30   \n",
      "79198    9377906289175510514  2017-06-30   \n",
      "79199    9377906289175510514  2017-06-30   \n",
      "\n",
      "                                     time_interval  travel_time  length  \\\n",
      "4316799  [2017-03-01 00:32:00,2017-03-01 00:34:00)          5.4      48   \n",
      "4317063  [2017-03-01 00:34:00,2017-03-01 00:36:00)          5.4      48   \n",
      "4316809  [2017-03-01 00:36:00,2017-03-01 00:38:00)          5.4      48   \n",
      "4316850  [2017-03-01 00:46:00,2017-03-01 00:48:00)          4.4      48   \n",
      "4316947  [2017-03-01 00:48:00,2017-03-01 00:50:00)          4.4      48   \n",
      "4316810  [2017-03-01 00:50:00,2017-03-01 00:52:00)          4.3      48   \n",
      "4316765  [2017-03-01 00:52:00,2017-03-01 00:54:00)          4.3      48   \n",
      "4317064  [2017-03-01 00:54:00,2017-03-01 00:56:00)          4.5      48   \n",
      "4317049  [2017-03-01 00:56:00,2017-03-01 00:58:00)          4.5      48   \n",
      "4316885  [2017-03-01 00:58:00,2017-03-01 01:00:00)          4.5      48   \n",
      "4317065  [2017-03-01 01:00:00,2017-03-01 01:02:00)          4.5      48   \n",
      "4317039  [2017-03-01 01:20:00,2017-03-01 01:22:00)          4.2      48   \n",
      "4316948  [2017-03-01 01:22:00,2017-03-01 01:24:00)          4.2      48   \n",
      "4316811  [2017-03-01 01:24:00,2017-03-01 01:26:00)          4.2      48   \n",
      "4317066  [2017-03-01 01:26:00,2017-03-01 01:28:00)          4.2      48   \n",
      "4316989  [2017-03-01 02:10:00,2017-03-01 02:12:00)          5.1      48   \n",
      "4316932  [2017-03-01 02:12:00,2017-03-01 02:14:00)          5.1      48   \n",
      "4316916  [2017-03-01 02:14:00,2017-03-01 02:16:00)          5.1      48   \n",
      "4316766  [2017-03-01 02:16:00,2017-03-01 02:18:00)          5.1      48   \n",
      "4316767  [2017-03-01 03:18:00,2017-03-01 03:20:00)          5.0      48   \n",
      "4317067  [2017-03-01 03:20:00,2017-03-01 03:22:00)          5.0      48   \n",
      "4316917  [2017-03-01 03:22:00,2017-03-01 03:24:00)          5.0      48   \n",
      "4316960  [2017-03-01 03:24:00,2017-03-01 03:26:00)          5.0      48   \n",
      "4316886  [2017-03-01 03:26:00,2017-03-01 03:28:00)          5.0      48   \n",
      "4316949  [2017-03-01 03:28:00,2017-03-01 03:30:00)          5.0      48   \n",
      "4316961  [2017-03-01 03:30:00,2017-03-01 03:32:00)          5.0      48   \n",
      "4316950  [2017-03-01 03:32:00,2017-03-01 03:34:00)          5.0      48   \n",
      "4317017  [2017-03-01 03:44:00,2017-03-01 03:46:00)          5.3      48   \n",
      "4316918  [2017-03-01 03:46:00,2017-03-01 03:48:00)          5.3      48   \n",
      "4316990  [2017-03-01 03:48:00,2017-03-01 03:50:00)          5.3      48   \n",
      "...                                            ...          ...     ...   \n",
      "79170    [2017-06-30 08:00:00,2017-06-30 08:02:00)          NaN      29   \n",
      "79171    [2017-06-30 08:02:00,2017-06-30 08:04:00)          NaN      29   \n",
      "79172    [2017-06-30 08:04:00,2017-06-30 08:06:00)          NaN      29   \n",
      "79173    [2017-06-30 08:06:00,2017-06-30 08:08:00)          NaN      29   \n",
      "79174    [2017-06-30 08:08:00,2017-06-30 08:10:00)          NaN      29   \n",
      "79175    [2017-06-30 08:10:00,2017-06-30 08:12:00)          NaN      29   \n",
      "79176    [2017-06-30 08:12:00,2017-06-30 08:14:00)          NaN      29   \n",
      "79177    [2017-06-30 08:14:00,2017-06-30 08:16:00)          NaN      29   \n",
      "79178    [2017-06-30 08:16:00,2017-06-30 08:18:00)          NaN      29   \n",
      "79179    [2017-06-30 08:18:00,2017-06-30 08:20:00)          NaN      29   \n",
      "79180    [2017-06-30 08:20:00,2017-06-30 08:22:00)          NaN      29   \n",
      "79181    [2017-06-30 08:22:00,2017-06-30 08:24:00)          NaN      29   \n",
      "79182    [2017-06-30 08:24:00,2017-06-30 08:26:00)          NaN      29   \n",
      "79183    [2017-06-30 08:26:00,2017-06-30 08:28:00)          NaN      29   \n",
      "79184    [2017-06-30 08:28:00,2017-06-30 08:30:00)          NaN      29   \n",
      "79185    [2017-06-30 08:30:00,2017-06-30 08:32:00)          NaN      29   \n",
      "79186    [2017-06-30 08:32:00,2017-06-30 08:34:00)          NaN      29   \n",
      "79187    [2017-06-30 08:34:00,2017-06-30 08:36:00)          NaN      29   \n",
      "79188    [2017-06-30 08:36:00,2017-06-30 08:38:00)          NaN      29   \n",
      "79189    [2017-06-30 08:38:00,2017-06-30 08:40:00)          NaN      29   \n",
      "79190    [2017-06-30 08:40:00,2017-06-30 08:42:00)          NaN      29   \n",
      "79191    [2017-06-30 08:42:00,2017-06-30 08:44:00)          NaN      29   \n",
      "79192    [2017-06-30 08:44:00,2017-06-30 08:46:00)          NaN      29   \n",
      "79193    [2017-06-30 08:46:00,2017-06-30 08:48:00)          NaN      29   \n",
      "79194    [2017-06-30 08:48:00,2017-06-30 08:50:00)          NaN      29   \n",
      "79195    [2017-06-30 08:50:00,2017-06-30 08:52:00)          NaN      29   \n",
      "79196    [2017-06-30 08:52:00,2017-06-30 08:54:00)          NaN      29   \n",
      "79197    [2017-06-30 08:54:00,2017-06-30 08:56:00)          NaN      29   \n",
      "79198    [2017-06-30 08:56:00,2017-06-30 08:58:00)          NaN      29   \n",
      "79199    [2017-06-30 08:58:00,2017-06-30 09:00:00)          NaN      29   \n",
      "\n",
      "         width  link_class  \n",
      "4316799      3           1  \n",
      "4317063      3           1  \n",
      "4316809      3           1  \n",
      "4316850      3           1  \n",
      "4316947      3           1  \n",
      "4316810      3           1  \n",
      "4316765      3           1  \n",
      "4317064      3           1  \n",
      "4317049      3           1  \n",
      "4316885      3           1  \n",
      "4317065      3           1  \n",
      "4317039      3           1  \n",
      "4316948      3           1  \n",
      "4316811      3           1  \n",
      "4317066      3           1  \n",
      "4316989      3           1  \n",
      "4316932      3           1  \n",
      "4316916      3           1  \n",
      "4316766      3           1  \n",
      "4316767      3           1  \n",
      "4317067      3           1  \n",
      "4316917      3           1  \n",
      "4316960      3           1  \n",
      "4316886      3           1  \n",
      "4316949      3           1  \n",
      "4316961      3           1  \n",
      "4316950      3           1  \n",
      "4317017      3           1  \n",
      "4316918      3           1  \n",
      "4316990      3           1  \n",
      "...        ...         ...  \n",
      "79170        9           1  \n",
      "79171        9           1  \n",
      "79172        9           1  \n",
      "79173        9           1  \n",
      "79174        9           1  \n",
      "79175        9           1  \n",
      "79176        9           1  \n",
      "79177        9           1  \n",
      "79178        9           1  \n",
      "79179        9           1  \n",
      "79180        9           1  \n",
      "79181        9           1  \n",
      "79182        9           1  \n",
      "79183        9           1  \n",
      "79184        9           1  \n",
      "79185        9           1  \n",
      "79186        9           1  \n",
      "79187        9           1  \n",
      "79188        9           1  \n",
      "79189        9           1  \n",
      "79190        9           1  \n",
      "79191        9           1  \n",
      "79192        9           1  \n",
      "79193        9           1  \n",
      "79194        9           1  \n",
      "79195        9           1  \n",
      "79196        9           1  \n",
      "79197        9           1  \n",
      "79198        9           1  \n",
      "79199        9           1  \n",
      "\n",
      "[7823975 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_date = feature_date.sort_values(['link_ID','time_interval'])\n",
    "print feature_date\n",
    "feature_date.to_csv('data/feature_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddBaseTimeFeature(df):\n",
    "    df['time_interval_begin'] = pd.to_datetime(df['time_interval'].map(lambda x: x[1:20]))  # 取出时间序列\n",
    "    df = df.drop(['date', 'time_interval'], axis=1)\n",
    "    df['time_interval_month'] = df['time_interval_begin'].map(lambda x: x.strftime('%m'))  # 月份\n",
    "    df['time_interval_day'] = df['time_interval_begin'].map(lambda x: x.day)  # 天数\n",
    "    df['time_interval_begin_hour'] = df['time_interval_begin'].map(lambda x: x.strftime('%H'))  # 小时\n",
    "    df['time_interval_minutes'] = df['time_interval_begin'].map(lambda x: x.strftime('%M'))  # 分钟\n",
    "    # Monday=1, Sunday=7\n",
    "    df['time_interval_week'] = df['time_interval_begin'].map(lambda x: x.weekday() + 1)  # 星期几\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/home/chenzh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     link_ID  travel_time  length  width  link_class  \\\n",
      "0        3377906280028510514          5.4      48      3           1   \n",
      "1        3377906280028510514          5.4      48      3           1   \n",
      "2        3377906280028510514          5.4      48      3           1   \n",
      "3        3377906280028510514          4.4      48      3           1   \n",
      "4        3377906280028510514          4.4      48      3           1   \n",
      "5        3377906280028510514          4.3      48      3           1   \n",
      "6        3377906280028510514          4.3      48      3           1   \n",
      "7        3377906280028510514          4.5      48      3           1   \n",
      "8        3377906280028510514          4.5      48      3           1   \n",
      "9        3377906280028510514          4.5      48      3           1   \n",
      "10       3377906280028510514          4.5      48      3           1   \n",
      "11       3377906280028510514          4.2      48      3           1   \n",
      "12       3377906280028510514          4.2      48      3           1   \n",
      "13       3377906280028510514          4.2      48      3           1   \n",
      "14       3377906280028510514          4.2      48      3           1   \n",
      "15       3377906280028510514          5.1      48      3           1   \n",
      "16       3377906280028510514          5.1      48      3           1   \n",
      "17       3377906280028510514          5.1      48      3           1   \n",
      "18       3377906280028510514          5.1      48      3           1   \n",
      "19       3377906280028510514          5.0      48      3           1   \n",
      "20       3377906280028510514          5.0      48      3           1   \n",
      "21       3377906280028510514          5.0      48      3           1   \n",
      "22       3377906280028510514          5.0      48      3           1   \n",
      "23       3377906280028510514          5.0      48      3           1   \n",
      "24       3377906280028510514          5.0      48      3           1   \n",
      "25       3377906280028510514          5.0      48      3           1   \n",
      "26       3377906280028510514          5.0      48      3           1   \n",
      "27       3377906280028510514          5.3      48      3           1   \n",
      "28       3377906280028510514          5.3      48      3           1   \n",
      "29       3377906280028510514          5.3      48      3           1   \n",
      "...                      ...          ...     ...    ...         ...   \n",
      "7823945  9377906289175510514          NaN      29      9           1   \n",
      "7823946  9377906289175510514          NaN      29      9           1   \n",
      "7823947  9377906289175510514          NaN      29      9           1   \n",
      "7823948  9377906289175510514          NaN      29      9           1   \n",
      "7823949  9377906289175510514          NaN      29      9           1   \n",
      "7823950  9377906289175510514          NaN      29      9           1   \n",
      "7823951  9377906289175510514          NaN      29      9           1   \n",
      "7823952  9377906289175510514          NaN      29      9           1   \n",
      "7823953  9377906289175510514          NaN      29      9           1   \n",
      "7823954  9377906289175510514          NaN      29      9           1   \n",
      "7823955  9377906289175510514          NaN      29      9           1   \n",
      "7823956  9377906289175510514          NaN      29      9           1   \n",
      "7823957  9377906289175510514          NaN      29      9           1   \n",
      "7823958  9377906289175510514          NaN      29      9           1   \n",
      "7823959  9377906289175510514          NaN      29      9           1   \n",
      "7823960  9377906289175510514          NaN      29      9           1   \n",
      "7823961  9377906289175510514          NaN      29      9           1   \n",
      "7823962  9377906289175510514          NaN      29      9           1   \n",
      "7823963  9377906289175510514          NaN      29      9           1   \n",
      "7823964  9377906289175510514          NaN      29      9           1   \n",
      "7823965  9377906289175510514          NaN      29      9           1   \n",
      "7823966  9377906289175510514          NaN      29      9           1   \n",
      "7823967  9377906289175510514          NaN      29      9           1   \n",
      "7823968  9377906289175510514          NaN      29      9           1   \n",
      "7823969  9377906289175510514          NaN      29      9           1   \n",
      "7823970  9377906289175510514          NaN      29      9           1   \n",
      "7823971  9377906289175510514          NaN      29      9           1   \n",
      "7823972  9377906289175510514          NaN      29      9           1   \n",
      "7823973  9377906289175510514          NaN      29      9           1   \n",
      "7823974  9377906289175510514          NaN      29      9           1   \n",
      "\n",
      "        time_interval_begin time_interval_month  time_interval_day  \\\n",
      "0       2017-03-01 00:32:00                  03                  1   \n",
      "1       2017-03-01 00:34:00                  03                  1   \n",
      "2       2017-03-01 00:36:00                  03                  1   \n",
      "3       2017-03-01 00:46:00                  03                  1   \n",
      "4       2017-03-01 00:48:00                  03                  1   \n",
      "5       2017-03-01 00:50:00                  03                  1   \n",
      "6       2017-03-01 00:52:00                  03                  1   \n",
      "7       2017-03-01 00:54:00                  03                  1   \n",
      "8       2017-03-01 00:56:00                  03                  1   \n",
      "9       2017-03-01 00:58:00                  03                  1   \n",
      "10      2017-03-01 01:00:00                  03                  1   \n",
      "11      2017-03-01 01:20:00                  03                  1   \n",
      "12      2017-03-01 01:22:00                  03                  1   \n",
      "13      2017-03-01 01:24:00                  03                  1   \n",
      "14      2017-03-01 01:26:00                  03                  1   \n",
      "15      2017-03-01 02:10:00                  03                  1   \n",
      "16      2017-03-01 02:12:00                  03                  1   \n",
      "17      2017-03-01 02:14:00                  03                  1   \n",
      "18      2017-03-01 02:16:00                  03                  1   \n",
      "19      2017-03-01 03:18:00                  03                  1   \n",
      "20      2017-03-01 03:20:00                  03                  1   \n",
      "21      2017-03-01 03:22:00                  03                  1   \n",
      "22      2017-03-01 03:24:00                  03                  1   \n",
      "23      2017-03-01 03:26:00                  03                  1   \n",
      "24      2017-03-01 03:28:00                  03                  1   \n",
      "25      2017-03-01 03:30:00                  03                  1   \n",
      "26      2017-03-01 03:32:00                  03                  1   \n",
      "27      2017-03-01 03:44:00                  03                  1   \n",
      "28      2017-03-01 03:46:00                  03                  1   \n",
      "29      2017-03-01 03:48:00                  03                  1   \n",
      "...                     ...                 ...                ...   \n",
      "7823945 2017-06-30 08:00:00                  06                 30   \n",
      "7823946 2017-06-30 08:02:00                  06                 30   \n",
      "7823947 2017-06-30 08:04:00                  06                 30   \n",
      "7823948 2017-06-30 08:06:00                  06                 30   \n",
      "7823949 2017-06-30 08:08:00                  06                 30   \n",
      "7823950 2017-06-30 08:10:00                  06                 30   \n",
      "7823951 2017-06-30 08:12:00                  06                 30   \n",
      "7823952 2017-06-30 08:14:00                  06                 30   \n",
      "7823953 2017-06-30 08:16:00                  06                 30   \n",
      "7823954 2017-06-30 08:18:00                  06                 30   \n",
      "7823955 2017-06-30 08:20:00                  06                 30   \n",
      "7823956 2017-06-30 08:22:00                  06                 30   \n",
      "7823957 2017-06-30 08:24:00                  06                 30   \n",
      "7823958 2017-06-30 08:26:00                  06                 30   \n",
      "7823959 2017-06-30 08:28:00                  06                 30   \n",
      "7823960 2017-06-30 08:30:00                  06                 30   \n",
      "7823961 2017-06-30 08:32:00                  06                 30   \n",
      "7823962 2017-06-30 08:34:00                  06                 30   \n",
      "7823963 2017-06-30 08:36:00                  06                 30   \n",
      "7823964 2017-06-30 08:38:00                  06                 30   \n",
      "7823965 2017-06-30 08:40:00                  06                 30   \n",
      "7823966 2017-06-30 08:42:00                  06                 30   \n",
      "7823967 2017-06-30 08:44:00                  06                 30   \n",
      "7823968 2017-06-30 08:46:00                  06                 30   \n",
      "7823969 2017-06-30 08:48:00                  06                 30   \n",
      "7823970 2017-06-30 08:50:00                  06                 30   \n",
      "7823971 2017-06-30 08:52:00                  06                 30   \n",
      "7823972 2017-06-30 08:54:00                  06                 30   \n",
      "7823973 2017-06-30 08:56:00                  06                 30   \n",
      "7823974 2017-06-30 08:58:00                  06                 30   \n",
      "\n",
      "        time_interval_begin_hour time_interval_minutes  time_interval_week  \n",
      "0                             00                    32                   3  \n",
      "1                             00                    34                   3  \n",
      "2                             00                    36                   3  \n",
      "3                             00                    46                   3  \n",
      "4                             00                    48                   3  \n",
      "5                             00                    50                   3  \n",
      "6                             00                    52                   3  \n",
      "7                             00                    54                   3  \n",
      "8                             00                    56                   3  \n",
      "9                             00                    58                   3  \n",
      "10                            01                    00                   3  \n",
      "11                            01                    20                   3  \n",
      "12                            01                    22                   3  \n",
      "13                            01                    24                   3  \n",
      "14                            01                    26                   3  \n",
      "15                            02                    10                   3  \n",
      "16                            02                    12                   3  \n",
      "17                            02                    14                   3  \n",
      "18                            02                    16                   3  \n",
      "19                            03                    18                   3  \n",
      "20                            03                    20                   3  \n",
      "21                            03                    22                   3  \n",
      "22                            03                    24                   3  \n",
      "23                            03                    26                   3  \n",
      "24                            03                    28                   3  \n",
      "25                            03                    30                   3  \n",
      "26                            03                    32                   3  \n",
      "27                            03                    44                   3  \n",
      "28                            03                    46                   3  \n",
      "29                            03                    48                   3  \n",
      "...                          ...                   ...                 ...  \n",
      "7823945                       08                    00                   5  \n",
      "7823946                       08                    02                   5  \n",
      "7823947                       08                    04                   5  \n",
      "7823948                       08                    06                   5  \n",
      "7823949                       08                    08                   5  \n",
      "7823950                       08                    10                   5  \n",
      "7823951                       08                    12                   5  \n",
      "7823952                       08                    14                   5  \n",
      "7823953                       08                    16                   5  \n",
      "7823954                       08                    18                   5  \n",
      "7823955                       08                    20                   5  \n",
      "7823956                       08                    22                   5  \n",
      "7823957                       08                    24                   5  \n",
      "7823958                       08                    26                   5  \n",
      "7823959                       08                    28                   5  \n",
      "7823960                       08                    30                   5  \n",
      "7823961                       08                    32                   5  \n",
      "7823962                       08                    34                   5  \n",
      "7823963                       08                    36                   5  \n",
      "7823964                       08                    38                   5  \n",
      "7823965                       08                    40                   5  \n",
      "7823966                       08                    42                   5  \n",
      "7823967                       08                    44                   5  \n",
      "7823968                       08                    46                   5  \n",
      "7823969                       08                    48                   5  \n",
      "7823970                       08                    50                   5  \n",
      "7823971                       08                    52                   5  \n",
      "7823972                       08                    54                   5  \n",
      "7823973                       08                    56                   5  \n",
      "7823974                       08                    58                   5  \n",
      "\n",
      "[7823975 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_data = pd.read_csv('data/feature_data.csv')\n",
    "feature_data_date = AddBaseTimeFeature(feature_data)\n",
    "print feature_data_date\n",
    "feature_data_date.to_csv('data/feature_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "feature_data = pd.read_csv('data/feature_data.csv')\n",
    "test = feature_data.loc[(feature_data.time_interval_month == 6)&(feature_data.time_interval_begin_hour==8),: ]\n",
    "test.to_csv('data/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mode_function(df):\n",
    "    counts = mode(df)\n",
    "    return counts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               link_ID  travel_time  length  width  time_interval_begin  \\\n",
      "0  3377906280028510514          5.4      48      3  2017-03-01 00:32:00   \n",
      "1  3377906280028510514          5.4      48      3  2017-03-01 00:34:00   \n",
      "2  3377906280028510514          5.4      48      3  2017-03-01 00:36:00   \n",
      "3  3377906280028510514          4.4      48      3  2017-03-01 00:46:00   \n",
      "4  3377906280028510514          4.4      48      3  2017-03-01 00:48:00   \n",
      "\n",
      "   time_interval_month  time_interval_day  time_interval_begin_hour  \\\n",
      "0                    3                  1                         0   \n",
      "1                    3                  1                         0   \n",
      "2                    3                  1                         0   \n",
      "3                    3                  1                         0   \n",
      "4                    3                  1                         0   \n",
      "\n",
      "   time_interval_minutes  week_1  week_2  week_3  week_4  week_5  week_6  \\\n",
      "0                     32       0       0       1       0       0       0   \n",
      "1                     34       0       0       1       0       0       0   \n",
      "2                     36       0       0       1       0       0       0   \n",
      "3                     46       0       0       1       0       0       0   \n",
      "4                     48       0       0       1       0       0       0   \n",
      "\n",
      "   week_7  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "feature_data = pd.read_csv('data/feature_data.csv')\n",
    "feature_data['link_ID'] = feature_data['link_ID'].astype(str)\n",
    "# link_info_count = pd.read_csv('./pre_data/link_info_count.csv')\n",
    "# link_info_count['link_ID'] = link_info_count['link_ID'].astype(str)\n",
    "# feature_data = pd.merge(feature_data,link_info_count,on='link_ID',how='left')\n",
    "# link_class = pd.get_dummies(feature_data['link_class'],prefix='link_class')\n",
    "# int_count_onehot = pd.get_dummies(feature_data['in_count_'],prefix='in_count')\n",
    "# out_count_onehot = pd.get_dummies(feature_data['out_count_'],prefix='out_count')\n",
    "week = pd.get_dummies(feature_data['time_interval_week'],prefix='week')\n",
    "# time_interval_minutes = pd.get_dummies(feature_data['time_interval_minutes'],prefix='time_interval_minutes')\n",
    "# day = pd.get_dummies(feature_data['time_interval_day'],prefix='day')\n",
    "feature_data.drop(['time_interval_week','link_class'],inplace=True,axis=1)\n",
    "# linkId = pd.get_dummies(feature_data['link_ID'],prefix='link_id')\n",
    "feature_data = pd.concat([feature_data,week],axis=1)\n",
    "print feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4月8时\n",
    "train = feature_data.loc[(feature_data.time_interval_month == 4)&(feature_data.time_interval_begin_hour==8),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    # 4，7时数据，min大于等于i\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 4)&(feature_data.time_interval_begin_hour == 7)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train = pd.merge(train,tmp,on=['link_ID','time_interval_day'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4月18时\n",
    "train_ = feature_data.loc[(feature_data.time_interval_month == 4)&(feature_data.time_interval_begin_hour==18),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 4)&(feature_data.time_interval_begin_hour == 17)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train_ = pd.merge(train_,tmp,on=['link_ID','time_interval_day'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5月18时\n",
    "train__ = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour==18),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 17)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train__ = pd.merge(train__,tmp,on=['link_ID','time_interval_day'],how='left')\n",
    "# 5月9时\n",
    "train___ = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour==9),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 8)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train___ = pd.merge(train___,tmp,on=['link_ID','time_interval_day'],how='left')\n",
    "# 5月19时\n",
    "train____ = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour==19),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 18)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train____ = pd.merge(train____,tmp,on=['link_ID','time_interval_day'],how='left')\n",
    "# 5月7时\n",
    "train_____ = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour==7),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 6)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    train_____ = pd.merge(train_____,tmp,on=['link_ID','time_interval_day'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,train_,train_____,train___,train__,train____],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3月\n",
    "train_history = feature_data.loc[(feature_data.time_interval_month == 3),: ]\n",
    "train_history = train_history.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('mean_m', np.mean), ('median_m', np.median),\n",
    "                                ('mode_m', mode_function), ('std_m', np.std), ('max_m', np.max),('min_m', np.min)]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train,train_history,on=['link_ID','time_interval_minutes'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['speed_mode'] = train['length']  / train['mode_m']\n",
    "train['speed_median'] = train['length']  / train['median_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['mean_std'] = train['mean_m']  / train['std_m']\n",
    "train['max_min_distance'] = train['max_m'] - train['min_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690029, 70)\n"
     ]
    }
   ],
   "source": [
    "train_8 = feature_data.loc[(feature_data.time_interval_month == 3)&(feature_data.time_interval_begin_hour == 8),: ]\n",
    "train_8 = train_8.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('median_8_', np.median),('mode_8_', mode_function)]).reset_index()\n",
    "\n",
    "train = pd.merge(train,train_8,on=['link_ID','time_interval_minutes'],how='left')\n",
    "\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = np.log1p(train.pop('travel_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour==8),: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               link_ID  travel_time  length  width  time_interval_begin  \\\n",
      "0  3377906280028510514          3.9      48      3  2017-05-01 08:08:00   \n",
      "1  3377906280028510514          3.9      48      3  2017-05-01 08:10:00   \n",
      "2  3377906280028510514          3.9      48      3  2017-05-01 08:12:00   \n",
      "3  3377906280028510514          3.9      48      3  2017-05-01 08:14:00   \n",
      "4  3377906280028510514          4.0      48      3  2017-05-01 08:20:00   \n",
      "\n",
      "   time_interval_month  time_interval_day  time_interval_begin_hour  \\\n",
      "0                    5                  1                         8   \n",
      "1                    5                  1                         8   \n",
      "2                    5                  1                         8   \n",
      "3                    5                  1                         8   \n",
      "4                    5                  1                         8   \n",
      "\n",
      "   time_interval_minutes  week_1   ...     mode_m     std_m  max_m  min_m  \\\n",
      "0                      8       1   ...        5.0  2.599082   26.6    1.5   \n",
      "1                     10       1   ...        5.0  2.485984   27.9    2.9   \n",
      "2                     12       1   ...        5.0  2.239663   25.9    3.1   \n",
      "3                     14       1   ...        5.0  2.432370   27.3    3.1   \n",
      "4                     20       1   ...        5.0  2.107287   19.4    2.8   \n",
      "\n",
      "   speed_mode  speed_median  mean_std  max_min_distance  median_8_  mode_8_  \n",
      "0         9.6      9.056604  2.316484              25.1       5.10      4.0  \n",
      "1         9.6      9.056604  2.395125              25.0       5.85      5.0  \n",
      "2         9.6      8.888889  2.627748              22.8       5.95      4.4  \n",
      "3         9.6      8.888889  2.440053              24.2       5.90      4.1  \n",
      "4         9.6      8.888889  2.783090              16.6       5.00      4.0  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 7)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    # tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    test = pd.merge(test,tmp,on=['link_ID','time_interval_day'],how='left')\n",
    "\n",
    "\n",
    "test_history = feature_data.loc[(feature_data.time_interval_month == 4),: ]\n",
    "test_history = test_history.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('mean_m', np.mean), ('median_m', np.median),\n",
    "                                ('mode_m', mode_function), ('std_m', np.std), ('max_m', np.max),('min_m', np.min)]).reset_index()\n",
    "# test_history['median_mode'] = 0.5 * test_history['mode_m'] + 0.5 * test_history['median_m']\n",
    "test = pd.merge(test,test_history,on=['link_ID','time_interval_minutes'],how='left')\n",
    "\n",
    "# test['speed_max'] = test['length']  / test['min_m']\n",
    "# test['speed_min'] = test['length']  / test['max_m']\n",
    "test['speed_mode'] = test['length']  / test['mode_m']\n",
    "test['speed_median'] = test['length']  / test['median_m']\n",
    "\n",
    "# test['120_speed'] = test['length']  / 120.0\n",
    "test['mean_std'] = test['mean_m']  / test['std_m']\n",
    "test['max_min_distance'] = test['max_m'] - test['min_m']\n",
    "\n",
    "test_8 = feature_data.loc[(feature_data.time_interval_month == 4)&(feature_data.time_interval_begin_hour == 8),: ]\n",
    "test_8 = test_8.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('median_8_', np.median),('mode_8_', mode_function)]).reset_index()\n",
    "\n",
    "test = pd.merge(test,test_8,on=['link_ID','time_interval_minutes'],how='left')\n",
    "\n",
    "print test.head()\n",
    "\n",
    "test_label = np.log1p(test.pop('travel_time'))\n",
    "\n",
    "train.drop(['time_interval_begin_hour','time_interval_month','time_interval_begin','std_m'],inplace=True,axis=1)\n",
    "test.drop(['time_interval_begin_hour','time_interval_month','time_interval_begin','std_m'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape_object(y,d):\n",
    "\n",
    "    g=1.0*np.sign(y-d)/d\n",
    "    h=1.0/d\n",
    "    return -g,h\n",
    "\n",
    "# 评价函数\n",
    "def mape(y,d):\n",
    "    c=d.get_label()\n",
    "    result=np.sum(np.abs(y-c)/c)/len(c)\n",
    "    return \"mape\",result\n",
    "\n",
    "# 评价函数ln形式\n",
    "def mape_ln(y,d):\n",
    "    c=d.get_label()\n",
    "    result=np.sum(np.abs(np.expm1(y)-np.abs(np.expm1(c)))/np.abs(np.expm1(c)))/len(c)\n",
    "    return \"mape\",result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mape:0.882369\n",
      "Will train until validation_0-mape hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mape:0.879446\n",
      "[2]\tvalidation_0-mape:0.876487\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-mape:0.882369\n",
      "\n",
      "{'reg_alpha': 1.0, 'colsample_bytree': 0.8, 'silent': True, 'colsample_bylevel': 1, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'missing': None, 'max_delta_step': 0, 'nthread': -1, 'base_score': 0.5, 'n_estimators': 1000, 'subsample': 0.8, 'reg_lambda': 0, 'seed': 9, 'min_child_weight': 5, 'objective': <function mape_object at 0x7fd841e46488>, 'max_depth': 11, 'gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "xlf = xgb.XGBRegressor(max_depth=11,\n",
    "                       learning_rate=0.01,\n",
    "                       n_estimators=1000,\n",
    "                       silent=True,\n",
    "                       objective=mape_object,\n",
    "                       gamma=0,\n",
    "                       min_child_weight=5,\n",
    "                       max_delta_step=0,\n",
    "                       subsample=0.8,\n",
    "                       colsample_bytree=0.8,\n",
    "                       colsample_bylevel=1,\n",
    "                       reg_alpha=1e0,\n",
    "                       reg_lambda=0,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=9,\n",
    "                       missing=None)\n",
    "\n",
    "\n",
    "xlf.fit(train.values, train_label.values, eval_metric=mape_ln, verbose=True, eval_set=[(test.values, test_label.values)],early_stopping_rounds=2)\n",
    "print xlf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               link_ID  travel_time  length  width  time_interval_begin  \\\n",
      "0  3377906280028510514          NaN      48      3  2017-06-01 08:00:00   \n",
      "1  3377906280028510514          NaN      48      3  2017-06-01 08:02:00   \n",
      "2  3377906280028510514          NaN      48      3  2017-06-01 08:04:00   \n",
      "3  3377906280028510514          NaN      48      3  2017-06-01 08:06:00   \n",
      "4  3377906280028510514          NaN      48      3  2017-06-01 08:08:00   \n",
      "\n",
      "   time_interval_month  time_interval_day  time_interval_begin_hour  \\\n",
      "0                    6                  1                         8   \n",
      "1                    6                  1                         8   \n",
      "2                    6                  1                         8   \n",
      "3                    6                  1                         8   \n",
      "4                    6                  1                         8   \n",
      "\n",
      "   time_interval_minutes  week_1   ...     mode_m     std_m  max_m  min_m  \\\n",
      "0                      0       0   ...        5.0  2.477008   22.6    2.1   \n",
      "1                      2       0   ...        5.4  2.768196   30.0    2.1   \n",
      "2                      4       0   ...        5.0  2.493873   19.8    1.8   \n",
      "3                      6       0   ...        5.2  4.757000   90.9    1.8   \n",
      "4                      8       0   ...        5.0  2.941153   27.6    1.8   \n",
      "\n",
      "   speed_mode  speed_median  mean_std  max_min_distance  median_8_  mode_8_  \n",
      "0    9.600000      8.727273  2.479662              20.5       5.35      4.5  \n",
      "1    8.888889      8.727273  2.247972              27.9       5.50      4.8  \n",
      "2    9.600000      8.421053  2.508226              18.0       5.45      4.9  \n",
      "3    9.230769      8.727273  1.332561              89.1       5.50      5.1  \n",
      "4    9.600000      8.727273  2.142424              25.8       5.50      3.9  \n",
      "\n",
      "[5 rows x 70 columns]\n",
      "(118800, 4)\n",
      "link_ID          0\n",
      "date             0\n",
      "time_interval    0\n",
      "travel_time      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sub = feature_data.loc[(feature_data.time_interval_month == 6)&(feature_data.time_interval_begin_hour==8),: ]\n",
    "for i in [58,48,38,28,18,8,0]:\n",
    "    tmp = feature_data.loc[(feature_data.time_interval_month == 6)&(feature_data.time_interval_begin_hour == 7)&(feature_data.time_interval_minutes >= i),:]\n",
    "    tmp = tmp.groupby(['link_ID', 'time_interval_day'])[\n",
    "            'travel_time'].agg([('mean_%d' % (i), np.mean), ('median_%d' % (i), np.median),\n",
    "                                ('mode_%d' % (i), mode_function), ('std_%d' % (i), np.std), ('max_%d' % (i), np.max),('min_%d' % (i), np.min)]).reset_index()\n",
    "    tmp['std_%d' % (i)] = tmp['std_%d' % (i)].fillna(0)\n",
    "    sub = pd.merge(sub,tmp,on=['link_ID','time_interval_day'],how='left')\n",
    "\n",
    "sub_history = feature_data.loc[(feature_data.time_interval_month == 5),: ]\n",
    "sub_history = sub_history.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('mean_m', np.mean), ('median_m', np.median),\n",
    "                                ('mode_m', mode_function), ('std_m', np.std), ('max_m', np.max),('min_m', np.min)]).reset_index()\n",
    "# sub_history['median_mode'] = 0.5 * sub_history['mode_m'] + 0.5 * sub_history['median_m']\n",
    "\n",
    "sub = pd.merge(sub,sub_history,on=['link_ID','time_interval_minutes'],how='left')\n",
    "# sub['speed_max'] = sub['length'] / sub['min_m']\n",
    "# sub['speed_min'] = sub['length'] / sub['max_m']\n",
    "sub['speed_mode'] = sub['length'] / sub['mode_m']\n",
    "sub['speed_median'] = sub['length'] / sub['median_m']\n",
    "\n",
    "# sub['120_speed'] = sub['length'] / 120.0\n",
    "\n",
    "sub['mean_std'] = sub['mean_m']  / sub['std_m']\n",
    "sub['max_min_distance'] = sub['max_m'] - sub['min_m']\n",
    "\n",
    "sub_history_8 = feature_data.loc[(feature_data.time_interval_month == 5)&(feature_data.time_interval_begin_hour == 8),: ]\n",
    "sub_history_8 = sub_history_8.groupby(['link_ID', 'time_interval_minutes'])[\n",
    "            'travel_time'].agg([('median_8_', np.median),('mode_8_', mode_function)]).reset_index()\n",
    "\n",
    "sub = pd.merge(sub,sub_history_8,on=['link_ID','time_interval_minutes'],how='left')\n",
    "\n",
    "print sub.head()\n",
    "\n",
    "sub_label = np.log1p(sub.pop('travel_time'))\n",
    "\n",
    "sub.drop(['time_interval_begin_hour','time_interval_month','time_interval_begin','std_m'],inplace=True,axis=1)\n",
    "\n",
    "result = xlf.predict(sub.values)\n",
    "\n",
    "travel_time = pd.DataFrame({'travel_time':list(result)})\n",
    "sub_demo = pd.read_table(u'data/gy_contest_result_template.txt',header=None,sep='#')\n",
    "\n",
    "sub_demo.columns = ['link_ID','date','time_interval','travel_time']\n",
    "sub_demo = sub_demo.sort_values(['link_ID','time_interval']).reset_index()\n",
    "del sub_demo['index']\n",
    "\n",
    "del sub_demo['travel_time']\n",
    "tt = pd.concat([sub_demo,travel_time],axis=1)\n",
    "# tt = tt.fillna(0)\n",
    "tt['travel_time'] = np.round(np.expm1(tt['travel_time']),6)\n",
    "tt[['link_ID','date','time_interval','travel_time']].to_csv('./mapodoufu_2017-08-12.txt',sep='#',index=False,header=False)\n",
    "print tt[['link_ID','date','time_interval','travel_time']].shape\n",
    "print tt[['link_ID','date','time_interval','travel_time']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
