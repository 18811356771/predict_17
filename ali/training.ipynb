{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析提取特征\n",
    "- data/com_training.txt：预处理过后的数据\n",
    " - 2017.03.01-2017.07.31\n",
    " - [6, 7, 8, 13, 14, 15, 16, 17, 18]\n",
    " - log1p处理\n",
    " - 填充nan值\n",
    " - 5049000 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_ID</th>\n",
       "      <th>date</th>\n",
       "      <th>time_interval_begin</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>imputation1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:00:00</td>\n",
       "      <td>1.647113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:02:00</td>\n",
       "      <td>1.656736</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:04:00</td>\n",
       "      <td>1.658209</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:06:00</td>\n",
       "      <td>1.663662</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:08:00</td>\n",
       "      <td>1.672619</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:10:00</td>\n",
       "      <td>1.629241</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:12:00</td>\n",
       "      <td>1.629241</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:14:00</td>\n",
       "      <td>1.629241</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:16:00</td>\n",
       "      <td>1.689991</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2017-03-01 06:18:00</td>\n",
       "      <td>1.695072</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               link_ID        date time_interval_begin  travel_time  \\\n",
       "0  3377906280028510514  2017-03-01 2017-03-01 06:00:00     1.647113   \n",
       "1  3377906280028510514  2017-03-01 2017-03-01 06:02:00     1.656736   \n",
       "2  3377906280028510514  2017-03-01 2017-03-01 06:04:00     1.658209   \n",
       "3  3377906280028510514  2017-03-01 2017-03-01 06:06:00     1.663662   \n",
       "4  3377906280028510514  2017-03-01 2017-03-01 06:08:00     1.672619   \n",
       "5  3377906280028510514  2017-03-01 2017-03-01 06:10:00     1.629241   \n",
       "6  3377906280028510514  2017-03-01 2017-03-01 06:12:00     1.629241   \n",
       "7  3377906280028510514  2017-03-01 2017-03-01 06:14:00     1.629241   \n",
       "8  3377906280028510514  2017-03-01 2017-03-01 06:16:00     1.689991   \n",
       "9  3377906280028510514  2017-03-01 2017-03-01 06:18:00     1.695072   \n",
       "\n",
       "  imputation1  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  \n",
       "5       False  \n",
       "6       False  \n",
       "7       False  \n",
       "8        True  \n",
       "9        True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../TimeSeriesPrediction/data/com_training.txt', delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间特征\n",
    "def create_lagging(df, df_original, i):\n",
    "    df1 = df_original.copy()\n",
    "    df1['time_interval_begin'] = df1['time_interval_begin'] + pd.DateOffset(minutes=i * 2)\n",
    "    df1 = df1.rename(columns={'travel_time': 'lagging' + str(i)})\n",
    "    df2 = pd.merge(df, df1[['link_ID', 'time_interval_begin', 'lagging' + str(i)]],\n",
    "                   on=['link_ID', 'time_interval_begin'],\n",
    "                   how='left')\n",
    "    return df2\n",
    "\n",
    "df1 = create_lagging(df, df, 1)\n",
    "\n",
    "for i in range(2, 6):\n",
    "    df1 = create_lagging(df1, df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 长、宽特征\n",
    "link_infos = pd.read_csv('data/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops = pd.read_csv('data/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops['in_links'] = link_tops['in_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops['out_links'] = link_tops['out_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops = link_tops.fillna(0)\n",
    "link_infos = pd.merge(link_infos, link_tops, on=['link_ID'], how='left')\n",
    "link_infos['links_num'] = link_infos[\"in_links\"].astype('str') + \",\" + link_infos[\"out_links\"].astype('str')\n",
    "link_infos['area'] = link_infos['length'] * link_infos['width']\n",
    "df2 = pd.merge(df1, link_infos[['link_ID', 'length', 'width', 'links_num', 'area']], on=['link_ID'], how='left')\n",
    "\n",
    "# links_num feature\n",
    "df2.loc[df2['links_num'].isin(['0.0,2.0', '2.0,0.0', '1.0,0.0']), 'links_num'] = 'other'\n",
    "# df.boxplot(by=['links_num'], column='travel_time')\n",
    "# plt.show()\n",
    "\n",
    "# vacation feature\n",
    "df2.loc[df2['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 1\n",
    "df2.loc[~df2['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 0\n",
    "\n",
    "# minute_series for CV\n",
    "# 早午晚都编码成0-178\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 6) * 60\n",
    "\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 13) * 60\n",
    "\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 16) * 60\n",
    "\n",
    "# day_of_week_en feature\n",
    "df2['day_of_week'] = df2['time_interval_begin'].map(lambda x: x.weekday() + 1)\n",
    "df2.loc[df2['day_of_week'].isin([1, 2, 3]), 'day_of_week_en'] = 1\n",
    "df2.loc[df2['day_of_week'].isin([4, 5]), 'day_of_week_en'] = 2\n",
    "df2.loc[df2['day_of_week'].isin([6, 7]), 'day_of_week_en'] = 3\n",
    "\n",
    "# hour_en feature\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'hour_en'] = 1\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'hour_en'] = 2\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'hour_en'] = 3\n",
    "\n",
    "# week_hour feature\n",
    "df2['week_hour'] = df2[\"day_of_week_en\"].astype('str') + \",\" + df2[\"hour_en\"].astype('str')\n",
    "\n",
    "# df2.boxplot(by=['week_hour'], column='travel_time')\n",
    "# plt.show()\n",
    "\n",
    "df2 = pd.get_dummies(df2, columns=['week_hour', 'links_num', 'width'])\n",
    "\n",
    "# ID Label Encode\n",
    "def mean_time(group):\n",
    "    group['link_ID_en'] = group['travel_time'].mean()\n",
    "    return group\n",
    "\n",
    "df2 = df2.groupby('link_ID').apply(mean_time)\n",
    "sorted_link = np.sort(df2['link_ID_en'].unique())\n",
    "df2['link_ID_en'] = df2['link_ID_en'].map(lambda x: np.argmin(x >= sorted_link))\n",
    "# df.boxplot(by=['link_ID_en'], column='travel_time')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv('../../TimeSeriesPrediction/data/training.txt', header=True, index=None, sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgboost_submit(df, params):\n",
    "    train_df = df.loc[df['time_interval_begin'] < pd.to_datetime('2017-07-01')]\n",
    "\n",
    "    train_df = train_df.dropna()\n",
    "    X = train_df[train_feature].values\n",
    "    y = train_df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    feature_vis(regressor, train_feature)\n",
    "    joblib.dump(regressor, 'model/xgbr.pkl')\n",
    "    print regressor\n",
    "    submission(train_feature, regressor, df, 'submission/xgbr1.txt', 'submission/xgbr2.txt', 'submission/xgbr3.txt',\n",
    "               'submission/xgbr4.txt')\n",
    "\n",
    "\n",
    "def fit_evaluate(df, df_test, params):\n",
    "    df = df.dropna()\n",
    "    X = df[train_feature].values\n",
    "    y = df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    df_test = df_test[valid_feature].values\n",
    "    valid_data = bucket_data(df_test)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 objective='reg:linear', subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=False, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    # feature_vis(regressor, train_feature)\n",
    "\n",
    "    return regressor, cross_valid(regressor, valid_data,\n",
    "                                  lagging=lagging), regressor.best_iteration, regressor.best_score\n",
    "\n",
    "\n",
    "def train(df, params, best, vis=False):\n",
    "    train1 = df.loc[df['time_interval_begin'] <= pd.to_datetime('2017-03-24')]  # 0301-0324\n",
    "    train2 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-03-24')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-04-18'))]  # 0324-0418\n",
    "    train3 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-04-18')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-05-12'))]  # 0418-0512\n",
    "    train4 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-05-12')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-06'))]  # 0512-0606\n",
    "    train5 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-06-06')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-30'))]  # 0606-0630\n",
    "\n",
    "    regressor, loss1, best_iteration1, best_score1 = fit_evaluate(pd.concat([train1, train2, train3, train4]), train5,\n",
    "                                                                  params)\n",
    "    print (best_iteration1, best_score1, loss1)\n",
    "\n",
    "    regressor, loss2, best_iteration2, best_score2 = fit_evaluate(pd.concat([train1, train2, train3, train5]), train4,\n",
    "                                                                  params)\n",
    "    print (best_iteration2, best_score2, loss2)\n",
    "\n",
    "    regressor, loss3, best_iteration3, best_score3 = fit_evaluate(pd.concat([train1, train2, train4, train5]), train3,\n",
    "                                                                  params)\n",
    "    print (best_iteration3, best_score3, loss3)\n",
    "\n",
    "    regressor, loss4, best_iteration4, best_score4 = fit_evaluate(pd.concat([train1, train3, train4, train5]), train2,\n",
    "                                                                  params)\n",
    "    print (best_iteration4, best_score4, loss4)\n",
    "\n",
    "    regressor, loss5, best_iteration5, best_score5 = fit_evaluate(pd.concat([train2, train3, train4, train5]), train1,\n",
    "                                                                  params)\n",
    "    print (best_iteration5, best_score5, loss5)\n",
    "\n",
    "    if vis:\n",
    "        xgb.plot_tree(regressor, num_trees=5)\n",
    "        results = regressor.evals_result()\n",
    "        epochs = len(results['validation_0']['rmse'])\n",
    "        x_axis = range(0, epochs)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('rmse Loss')\n",
    "        plt.ylim((0.2, 0.3))\n",
    "        plt.show()\n",
    "\n",
    "    loss = [loss1, loss2, loss3, loss4, loss5]\n",
    "    params['loss_std'] = np.std(loss)\n",
    "    params['loss'] = str(loss)\n",
    "    params['mean_loss'] = np.mean(loss)\n",
    "    params['n_estimators'] = str([best_iteration1, best_iteration2, best_iteration3, best_iteration4, best_iteration5])\n",
    "    params['best_score'] = str([best_score1, best_score2, best_score3, best_score4, best_score5])\n",
    "\n",
    "    print str(params)\n",
    "    if np.mean(loss) <= best:\n",
    "        best = np.mean(loss)\n",
    "        print \"best with: \" + str(params)\n",
    "#         feature_vis(regressor, train_feature)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'vacation', 'day_of_week_en', 'week_hour_1.0,1.0', 'week_hour_1.0,2.0', 'week_hour_1.0,3.0', 'week_hour_2.0,1.0', 'week_hour_2.0,2.0', 'week_hour_2.0,3.0', 'week_hour_3.0,1.0', 'week_hour_3.0,2.0', 'week_hour_3.0,3.0', 'links_num_0.0,1.0', 'links_num_1.0,1.0', 'links_num_1.0,2.0', 'links_num_1.0,3.0', 'links_num_1.0,4.0', 'links_num_2.0,1.0', 'links_num_2.0,2.0', 'links_num_3.0,1.0', 'links_num_4.0,1.0', 'links_num_other', 'width_3', 'width_6', 'width_9', 'width_12', 'width_15', 'link_ID_en', 'lagging5', 'lagging4', 'lagging3', 'lagging2', 'lagging1']\n",
      "['length', 'vacation', 'day_of_week_en', 'week_hour_1.0,1.0', 'week_hour_1.0,2.0', 'week_hour_1.0,3.0', 'week_hour_2.0,1.0', 'week_hour_2.0,2.0', 'week_hour_2.0,3.0', 'week_hour_3.0,1.0', 'week_hour_3.0,2.0', 'week_hour_3.0,3.0', 'links_num_0.0,1.0', 'links_num_1.0,1.0', 'links_num_1.0,2.0', 'links_num_1.0,3.0', 'links_num_1.0,4.0', 'links_num_2.0,1.0', 'links_num_2.0,2.0', 'links_num_3.0,1.0', 'links_num_4.0,1.0', 'links_num_other', 'width_3', 'width_6', 'width_9', 'width_12', 'width_15', 'link_ID_en', 'minute_series', 'travel_time']\n"
     ]
    }
   ],
   "source": [
    "# train_feature中包括lagging特征\n",
    "# valid_feature中不包括lagging特征，包括['minute_series', 'travel_time']\n",
    "\n",
    "lagging = 5\n",
    "df = pd.read_csv('../../TimeSeriesPrediction/data/training.txt', delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "lagging_feature = ['lagging%01d' % e for e in range(lagging, 0, -1)]  # 生成lagging特征\n",
    "base_feature = [x for x in df.columns.values.tolist() if x not in ['time_interval_begin', 'link_ID', 'link_ID_int',\n",
    "                                                                   'date', 'travel_time', 'imputation1',\n",
    "                                                                   'minute_series', 'area', 'hour_en', 'day_of_week']]\n",
    "base_feature = [x for x in base_feature if x not in lagging_feature]\n",
    "train_feature = list(base_feature)\n",
    "train_feature.extend(lagging_feature)\n",
    "valid_feature = list(base_feature)\n",
    "valid_feature.extend(['minute_series', 'travel_time'])\n",
    "print train_feature\n",
    "print valid_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.865125, 0.80786214674780721)\n",
      "(0, 0.866922, 0.8240044555484739)\n",
      "(0, 0.866211, 0.79502990524713169)\n",
      "(0, 0.866606, 0.79436080458845282)\n",
      "(0, 0.866445, 0.77696943932234408)\n",
      "{'loss': '[0.80786214674780721, 0.8240044555484739, 0.79502990524713169, 0.79436080458845282, 0.77696943932234408]', 'reg_alpha': 2, 'colsample_bytree': 0.6, 'learning_rate': 0.05, 'min_child_weight': 1, 'best_score': '[0.865125, 0.866922, 0.866211, 0.866606, 0.866445]', 'n_estimators': '[0, 0, 0, 0, 0]', 'subsample': 0.6, 'mean_loss': 0.79964535029084194, 'loss_std': 0.015648050661031108, 'max_depth': 7, 'gamma': 0}\n",
      "best with: {'loss': '[0.80786214674780721, 0.8240044555484739, 0.79502990524713169, 0.79436080458845282, 0.77696943932234408]', 'reg_alpha': 2, 'colsample_bytree': 0.6, 'learning_rate': 0.05, 'min_child_weight': 1, 'best_score': '[0.865125, 0.866922, 0.866211, 0.866606, 0.866445]', 'n_estimators': '[0, 0, 0, 0, 0]', 'subsample': 0.6, 'mean_loss': 0.79964535029084194, 'loss_std': 0.015648050661031108, 'max_depth': 7, 'gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "params_grid = {\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.6],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'max_depth': [7],\n",
    "    'min_child_weight': [1],\n",
    "    'reg_alpha': [2],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(params_grid)\n",
    "best = 1\n",
    "\n",
    "for params in grid:\n",
    "    best = train(df, params, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key为倒数第二位，value为余下特征\n",
    "def bucket_data(lines):  # lines是df[valid_feature].values\n",
    "    bucket = {}\n",
    "    for line in lines:\n",
    "        time_series = line[-2]  #　倒数第二位为time_series特征，0-178\n",
    "        bucket[time_series] = []  # bucket的keys为0-178\n",
    "    for line in lines:\n",
    "        time_series, y1 = line[-2:]  # 倒数第二位，y1最后一位\n",
    "        line = np.delete(line, -2, axis=0)  # 删除倒数第二位\n",
    "        bucket[time_series].append(line)  # key为倒数第二位，value为余下特征\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def cross_valid(regressor, bucket, lagging):\n",
    "    valid_loss = []\n",
    "    last = [[] for i in range(len(bucket[bucket.keys()[0]]))]\n",
    "    for time_series in sorted(bucket.keys(), key=float):\n",
    "        if time_series >= 120:\n",
    "            if int(time_series) in range(120, 120 + lagging * 2, 2):\n",
    "                last = np.concatenate((last, np.array(bucket[time_series], dtype=float)[:, -1].reshape(-1, 1)), axis=1)\n",
    "            else:\n",
    "                batch = np.array(bucket[time_series], dtype=float)\n",
    "                y = batch[:, -1]\n",
    "                batch = np.delete(batch, -1, axis=1)\n",
    "                batch = np.concatenate((batch, last), axis=1)\n",
    "                y_pre = regressor.predict(batch)\n",
    "                last = np.delete(last, 0, axis=1)\n",
    "                last = np.concatenate((last, y_pre.reshape(-1, 1)), axis=1)\n",
    "                loss = np.mean(abs(np.expm1(y) - np.expm1(y_pre)) / np.expm1(y))\n",
    "                valid_loss.append(loss)\n",
    "    # print 'day: %d loss: %f' % (int(day), day_loss)\n",
    "    return np.mean(valid_loss)\n",
    "\n",
    "\n",
    "def mape_ln(y, d):\n",
    "    c = d.get_label()\n",
    "    result = np.sum(np.abs((np.expm1(y) - np.expm1(c)) / np.expm1(c))) / len(c)\n",
    "    return \"mape\", result\n",
    "\n",
    "\n",
    "def feature_vis(regressor, train_feature):\n",
    "    importances = regressor.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    selected_features = [train_feature[e] for e in indices]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title(\"train_feature importances\")\n",
    "    plt.bar(range(len(train_feature)), importances[indices],\n",
    "            color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(len(selected_features)), selected_features, rotation=70)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------Submission ---------------------------------------------\n",
    "\n",
    "\n",
    "def submission(train_feature, regressor, df, file1, file2, file3, file4):\n",
    "    test_df = df.loc[((df['time_interval_begin'].dt.year == 2017) & (df['time_interval_begin'].dt.month == 7)\n",
    "                      & (df['time_interval_begin'].dt.hour.isin([7, 14, 17])) & (\n",
    "                          df['time_interval_begin'].dt.minute == 58))].copy()\n",
    "\n",
    "    test_df['lagging5'] = test_df['lagging4']\n",
    "    test_df['lagging4'] = test_df['lagging3']\n",
    "    test_df['lagging3'] = test_df['lagging2']\n",
    "    test_df['lagging2'] = test_df['lagging1']\n",
    "    test_df['lagging1'] = test_df['travel_time']\n",
    "\n",
    "    with open(file1, 'w'):\n",
    "        pass\n",
    "    with open(file2, 'w'):\n",
    "        pass\n",
    "    with open(file3, 'w'):\n",
    "        pass\n",
    "    with open(file4, 'w'):\n",
    "        pass\n",
    "\n",
    "    for i in range(30):\n",
    "        test_X = test_df[train_feature]\n",
    "        y_prediction = regressor.predict(test_X.values)\n",
    "\n",
    "        test_df['lagging5'] = test_df['lagging4']\n",
    "        test_df['lagging4'] = test_df['lagging3']\n",
    "        test_df['lagging3'] = test_df['lagging2']\n",
    "        test_df['lagging2'] = test_df['lagging1']\n",
    "        test_df['lagging1'] = y_prediction\n",
    "\n",
    "        test_df['predicted'] = np.expm1(y_prediction)\n",
    "        test_df['time_interval_begin'] = test_df['time_interval_begin'] + pd.DateOffset(minutes=2)\n",
    "        test_df['time_interval'] = test_df['time_interval_begin'].map(\n",
    "            lambda x: '[' + str(x) + ',' + str(x + pd.DateOffset(minutes=2)) + ')')\n",
    "        test_df.time_interval = test_df.time_interval.astype(object)\n",
    "        if i < 7:\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file1, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        elif (7 <= i) and (i < 14):\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file2, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        elif (14 <= i) and (i < 22):\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file3, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        else:\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file4, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.99822675145421"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.80786214674780721+0.8240044555484739+0.79502990524713169+0.79436080458845282+0.77696943932234408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20035464970915806"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- 3.99822675145421/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key为倒数第二位，value为余下特征\n",
    "def bucket_data(lines):  # lines是df[valid_feature].values\n",
    "    bucket = {}\n",
    "    for line in lines:\n",
    "        time_series = line[-2]  #　倒数第二位为time_series特征，0-178\n",
    "        bucket[time_series] = []  # bucket的keys为0-178\n",
    "    for line in lines:\n",
    "        time_series, y1 = line[-2:]  # 倒数第二位，y1最后一位\n",
    "        line = np.delete(line, -2, axis=0)  # 删除倒数第二位\n",
    "        bucket[time_series].append(line)  # key为倒数第二位，value为余下特征\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def cross_valid(regressor, bucket, lagging):\n",
    "    valid_loss = []\n",
    "    last = [[] for i in range(len(bucket[bucket.keys()[0]]))]  # 有多少个数据\n",
    "    for time_series in sorted(bucket.keys(), key=float):\n",
    "        if time_series >= 120:  # 增加lagging特征\n",
    "            if int(time_series) in range(120, 120 + lagging * 2, 2):\n",
    "                last = np.concatenate((last, np.array(bucket[time_series], dtype=float)[:, -1].reshape(-1, 1)), axis=1)\n",
    "            else:\n",
    "                batch = np.array(bucket[time_series], dtype=float)  # 取出特征的值\n",
    "                y = batch[:, -1]  # travel_time\n",
    "                batch = np.delete(batch, -1, axis=1)  # 删除最后一行travel_time\n",
    "                batch = np.concatenate((batch, last), axis=1)\n",
    "                y_pre = regressor.predict(batch)\n",
    "                last = np.delete(last, 0, axis=1)\n",
    "                last = np.concatenate((last, y_pre.reshape(-1, 1)), axis=1)\n",
    "                loss = np.mean(abs(np.expm1(y) - np.expm1(y_pre)) / np.expm1(y))\n",
    "                valid_loss.append(loss)\n",
    "    # print 'day: %d loss: %f' % (int(day), day_loss)\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = train1[valid_feature].values\n",
    "valid_data = bucket_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = df.loc[df['time_interval_begin'] <= pd.to_datetime('2017-03-24')]  # 0301-0324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = np.array(valid_data[valid_data.keys()[0]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 122, 124, 126, 128]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(120, 120 + 5 * 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last = np.concatenate((last, np.array(bucket[time_series], dtype=float)[:, -1].reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last = np.concatenate((last, np.array(valid_data[valid_data.keys()[70]], dtype=float)[:, -1].reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.93703611,  1.93703611],\n",
       "       [ 1.98123143,  1.98123143],\n",
       "       [ 1.80828877,  1.80828877],\n",
       "       ..., \n",
       "       [ 1.22377543,  1.22377543],\n",
       "       [ 1.60943791,  1.60943791],\n",
       "       [ 1.45861502,  1.45861502]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((last, np.array(valid_data[valid_data.keys()[70]], dtype=float)[:, -1].reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.93703611],\n",
       "       [ 1.98123143],\n",
       "       [ 1.80828877],\n",
       "       ..., \n",
       "       [ 1.22377543],\n",
       "       [ 1.60943791],\n",
       "       [ 1.45861502]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(last, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
