{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析提取特征\n",
    "- data/com_training.txt：预处理过后的数据\n",
    " - 2017.03.01-2017.07.31\n",
    " - [6, 7, 8, 13, 14, 15, 16, 17, 18]\n",
    " - log1p处理\n",
    " - 填充nan值\n",
    " - 5049000 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f194c18e4984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File data/com_training.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a0dc5f16cdfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/com_training.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_interval_begin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'link_ID'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zydar\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zydar\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zydar\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zydar\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zydar\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File data/com_training.txt does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/com_training.txt', delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间特征\n",
    "def create_lagging(df, df_original, i):\n",
    "    df1 = df_original.copy()\n",
    "    df1['time_interval_begin'] = df1['time_interval_begin'] + pd.DateOffset(minutes=i * 2)\n",
    "    df1 = df1.rename(columns={'travel_time': 'lagging' + str(i)})\n",
    "    df2 = pd.merge(df, df1[['link_ID', 'time_interval_begin', 'lagging' + str(i)]],\n",
    "                   on=['link_ID', 'time_interval_begin'],\n",
    "                   how='left')\n",
    "    return df2\n",
    "\n",
    "df1 = create_lagging(df, df, 1)\n",
    "\n",
    "for i in range(2, 6):\n",
    "    df1 = create_lagging(df1, df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 长、宽特征\n",
    "link_infos = pd.read_csv('data/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops = pd.read_csv('data/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops['in_links'] = link_tops['in_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops['out_links'] = link_tops['out_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops = link_tops.fillna(0)\n",
    "link_infos = pd.merge(link_infos, link_tops, on=['link_ID'], how='left')\n",
    "link_infos['links_num'] = link_infos[\"in_links\"].astype('str') + \",\" + link_infos[\"out_links\"].astype('str')\n",
    "link_infos['area'] = link_infos['length'] * link_infos['width']\n",
    "df2 = pd.merge(df1, link_infos[['link_ID', 'length', 'width', 'links_num', 'area']], on=['link_ID'], how='left')\n",
    "\n",
    "# links_num feature\n",
    "df2.loc[df2['links_num'].isin(['0.0,2.0', '2.0,0.0', '1.0,0.0']), 'links_num'] = 'other'\n",
    "# df.boxplot(by=['links_num'], column='travel_time')\n",
    "# plt.show()\n",
    "\n",
    "# vacation feature\n",
    "df2.loc[df2['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 1\n",
    "df2.loc[~df2['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 0\n",
    "\n",
    "# minute_series for CV\n",
    "# 早午晚都编码成0-178\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 6) * 60\n",
    "\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 13) * 60\n",
    "\n",
    "df2.loc[df2['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'minute_series'] = \\\n",
    "    df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 16) * 60\n",
    "\n",
    "# day_of_week_en feature\n",
    "df2['day_of_week'] = df2['time_interval_begin'].map(lambda x: x.weekday() + 1)\n",
    "df2.loc[df2['day_of_week'].isin([1, 2, 3]), 'day_of_week_en'] = 1\n",
    "df2.loc[df2['day_of_week'].isin([4, 5]), 'day_of_week_en'] = 2\n",
    "df2.loc[df2['day_of_week'].isin([6, 7]), 'day_of_week_en'] = 3\n",
    "\n",
    "# hour_en feature\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'hour_en'] = 1\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'hour_en'] = 2\n",
    "df2.loc[df['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'hour_en'] = 3\n",
    "\n",
    "# week_hour feature\n",
    "df2['week_hour'] = df2[\"day_of_week_en\"].astype('str') + \",\" + df2[\"hour_en\"].astype('str')\n",
    "\n",
    "# df2.boxplot(by=['week_hour'], column='travel_time')\n",
    "# plt.show()\n",
    "\n",
    "df2 = pd.get_dummies(df2, columns=['week_hour', 'links_num', 'width'])\n",
    "\n",
    "# ID Label Encode\n",
    "def mean_time(group):\n",
    "    group['link_ID_en'] = group['travel_time'].mean()\n",
    "    return group\n",
    "\n",
    "df2 = df2.groupby('link_ID').apply(mean_time)\n",
    "sorted_link = np.sort(df2['link_ID_en'].unique())\n",
    "df2['link_ID_en'] = df2['link_ID_en'].map(lambda x: np.argmin(x >= sorted_link))\n",
    "# df.boxplot(by=['link_ID_en'], column='travel_time')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv('data/training.txt', header=True, index=None, sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgboost_submit(df, params):\n",
    "    train_df = df.loc[df['time_interval_begin'] < pd.to_datetime('2017-07-01')]\n",
    "\n",
    "    train_df = train_df.dropna()\n",
    "    X = train_df[train_feature].values\n",
    "    y = train_df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    feature_vis(regressor, train_feature)\n",
    "    joblib.dump(regressor, 'model/xgbr.pkl')\n",
    "    print regressor\n",
    "    submission(train_feature, regressor, df, 'submission/xgbr1.txt', 'submission/xgbr2.txt', 'submission/xgbr3.txt',\n",
    "               'submission/xgbr4.txt')\n",
    "\n",
    "\n",
    "def fit_evaluate(df, df_test, params):\n",
    "    df = df.dropna()\n",
    "    X = df[train_feature].values\n",
    "    y = df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    df_test = df_test[valid_feature].values\n",
    "    valid_data = bucket_data(df_test)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 objective='reg:linear', subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=False, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    # feature_vis(regressor, train_feature)\n",
    "\n",
    "    return regressor, cross_valid(regressor, valid_data,\n",
    "                                  lagging=lagging), regressor.best_iteration, regressor.best_score\n",
    "\n",
    "\n",
    "def train(df, params, best, vis=False):\n",
    "    train1 = df.loc[df['time_interval_begin'] <= pd.to_datetime('2017-03-24')]  # 0301-0324\n",
    "    train2 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-03-24')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-04-18'))]  # 0324-0418\n",
    "    train3 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-04-18')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-05-12'))]  # 0418-0512\n",
    "    train4 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-05-12')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-06'))]  # 0512-0606\n",
    "    train5 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-06-06')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-30'))]  # 0606-0630\n",
    "\n",
    "    regressor, loss1, best_iteration1, best_score1 = fit_evaluate(pd.concat([train1, train2, train3, train4]), train5,\n",
    "                                                                  params)\n",
    "    print (best_iteration1, best_score1, loss1)\n",
    "\n",
    "    regressor, loss2, best_iteration2, best_score2 = fit_evaluate(pd.concat([train1, train2, train3, train5]), train4,\n",
    "                                                                  params)\n",
    "    print (best_iteration2, best_score2, loss2)\n",
    "\n",
    "    regressor, loss3, best_iteration3, best_score3 = fit_evaluate(pd.concat([train1, train2, train4, train5]), train3,\n",
    "                                                                  params)\n",
    "    print (best_iteration3, best_score3, loss3)\n",
    "\n",
    "    regressor, loss4, best_iteration4, best_score4 = fit_evaluate(pd.concat([train1, train3, train4, train5]), train2,\n",
    "                                                                  params)\n",
    "    print (best_iteration4, best_score4, loss4)\n",
    "\n",
    "    regressor, loss5, best_iteration5, best_score5 = fit_evaluate(pd.concat([train2, train3, train4, train5]), train1,\n",
    "                                                                  params)\n",
    "    print (best_iteration5, best_score5, loss5)\n",
    "\n",
    "    if vis:\n",
    "        xgb.plot_tree(regressor, num_trees=5)\n",
    "        results = regressor.evals_result()\n",
    "        epochs = len(results['validation_0']['rmse'])\n",
    "        x_axis = range(0, epochs)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('rmse Loss')\n",
    "        plt.ylim((0.2, 0.3))\n",
    "        plt.show()\n",
    "\n",
    "    loss = [loss1, loss2, loss3, loss4, loss5]\n",
    "    params['loss_std'] = np.std(loss)\n",
    "    params['loss'] = str(loss)\n",
    "    params['mean_loss'] = np.mean(loss)\n",
    "    params['n_estimators'] = str([best_iteration1, best_iteration2, best_iteration3, best_iteration4, best_iteration5])\n",
    "    params['best_score'] = str([best_score1, best_score2, best_score3, best_score4, best_score5])\n",
    "\n",
    "    print str(params)\n",
    "    if np.mean(loss) <= best:\n",
    "        best = np.mean(loss)\n",
    "        print \"best with: \" + str(params)\n",
    "#         feature_vis(regressor, train_feature)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'vacation', 'day_of_week_en', 'week_hour_1.0,1.0', 'week_hour_1.0,2.0', 'week_hour_1.0,3.0', 'week_hour_2.0,1.0', 'week_hour_2.0,2.0', 'week_hour_2.0,3.0', 'week_hour_3.0,1.0', 'week_hour_3.0,2.0', 'week_hour_3.0,3.0', 'links_num_0.0,1.0', 'links_num_1.0,1.0', 'links_num_1.0,2.0', 'links_num_1.0,3.0', 'links_num_1.0,4.0', 'links_num_2.0,1.0', 'links_num_2.0,2.0', 'links_num_3.0,1.0', 'links_num_4.0,1.0', 'links_num_other', 'width_3', 'width_6', 'width_9', 'width_12', 'width_15', 'link_ID_en', 'lagging5', 'lagging4', 'lagging3', 'lagging2', 'lagging1']\n"
     ]
    }
   ],
   "source": [
    "# train_feature中包括lagging特征\n",
    "# valid_feature中不包括lagging特征，包括['minute_series', 'travel_time']\n",
    "\n",
    "lagging = 5\n",
    "df = pd.read_csv('data/training.txt', delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "lagging_feature = ['lagging%01d' % e for e in range(lagging, 0, -1)]  # 生成lagging特征\n",
    "base_feature = [x for x in df.columns.values.tolist() if x not in ['time_interval_begin', 'link_ID', 'link_ID_int',\n",
    "                                                                   'date', 'travel_time', 'imputation1',\n",
    "                                                                   'minute_series', 'area', 'hour_en', 'day_of_week']]\n",
    "base_feature = [x for x in base_feature if x not in lagging_feature]\n",
    "train_feature = list(base_feature)\n",
    "train_feature.extend(lagging_feature)\n",
    "valid_feature = list(base_feature)\n",
    "valid_feature.extend(['minute_series', 'travel_time'])\n",
    "print train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.865125, 0.80786214674780721)\n",
      "(0, 0.866922, 0.8240044555484739)\n",
      "(0, 0.866211, 0.79502990524713169)\n",
      "(0, 0.866606, 0.79436080458845282)\n",
      "(0, 0.866445, 0.77696943932234408)\n",
      "{'loss': '[0.80786214674780721, 0.8240044555484739, 0.79502990524713169, 0.79436080458845282, 0.77696943932234408]', 'reg_alpha': 2, 'colsample_bytree': 0.6, 'learning_rate': 0.05, 'min_child_weight': 1, 'best_score': '[0.865125, 0.866922, 0.866211, 0.866606, 0.866445]', 'n_estimators': '[0, 0, 0, 0, 0]', 'subsample': 0.6, 'mean_loss': 0.79964535029084194, 'loss_std': 0.015648050661031108, 'max_depth': 7, 'gamma': 0}\n",
      "best with: {'loss': '[0.80786214674780721, 0.8240044555484739, 0.79502990524713169, 0.79436080458845282, 0.77696943932234408]', 'reg_alpha': 2, 'colsample_bytree': 0.6, 'learning_rate': 0.05, 'min_child_weight': 1, 'best_score': '[0.865125, 0.866922, 0.866211, 0.866606, 0.866445]', 'n_estimators': '[0, 0, 0, 0, 0]', 'subsample': 0.6, 'mean_loss': 0.79964535029084194, 'loss_std': 0.015648050661031108, 'max_depth': 7, 'gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "params_grid = {\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.6],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'max_depth': [7],\n",
    "    'min_child_weight': [1],\n",
    "    'reg_alpha': [2],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(params_grid)\n",
    "best = 1\n",
    "\n",
    "for params in grid:\n",
    "    best = train(df, params, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key为倒数第二位，value为余下特征\n",
    "def bucket_data(lines):  # lines是df[valid_feature].values\n",
    "    bucket = {}\n",
    "    for line in lines:\n",
    "        time_series = line[-2]  #　倒数第二位为time_series特征，0-178\n",
    "        bucket[time_series] = []  # bucket的keys为0-178\n",
    "    for line in lines:\n",
    "        time_series, y1 = line[-2:]  # 倒数第二位，y1最后一位\n",
    "        line = np.delete(line, -2, axis=0)  # 删除倒数第二位\n",
    "        bucket[time_series].append(line)  # key为倒数第二位，value为余下特征\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def cross_valid(regressor, bucket, lagging):\n",
    "    valid_loss = []\n",
    "    last = [[] for i in range(len(bucket[bucket.keys()[0]]))]\n",
    "    for time_series in sorted(bucket.keys(), key=float):\n",
    "        if time_series >= 120:\n",
    "            if int(time_series) in range(120, 120 + lagging * 2, 2):\n",
    "                last = np.concatenate((last, np.array(bucket[time_series], dtype=float)[:, -1].reshape(-1, 1)), axis=1)\n",
    "            else:\n",
    "                batch = np.array(bucket[time_series], dtype=float)\n",
    "                y = batch[:, -1]\n",
    "                batch = np.delete(batch, -1, axis=1)\n",
    "                batch = np.concatenate((batch, last), axis=1)\n",
    "                y_pre = regressor.predict(batch)\n",
    "                last = np.delete(last, 0, axis=1)\n",
    "                last = np.concatenate((last, y_pre.reshape(-1, 1)), axis=1)\n",
    "                loss = np.mean(abs(np.expm1(y) - np.expm1(y_pre)) / np.expm1(y))\n",
    "                valid_loss.append(loss)\n",
    "    # print 'day: %d loss: %f' % (int(day), day_loss)\n",
    "    return np.mean(valid_loss)\n",
    "\n",
    "\n",
    "def mape_ln(y, d):\n",
    "    c = d.get_label()\n",
    "    result = np.sum(np.abs((np.expm1(y) - np.expm1(c)) / np.expm1(c))) / len(c)\n",
    "    return \"mape\", result\n",
    "\n",
    "\n",
    "def feature_vis(regressor, train_feature):\n",
    "    importances = regressor.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    selected_features = [train_feature[e] for e in indices]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title(\"train_feature importances\")\n",
    "    plt.bar(range(len(train_feature)), importances[indices],\n",
    "            color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(len(selected_features)), selected_features, rotation=70)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------Submission ---------------------------------------------\n",
    "\n",
    "\n",
    "def submission(train_feature, regressor, df, file1, file2, file3, file4):\n",
    "    test_df = df.loc[((df['time_interval_begin'].dt.year == 2017) & (df['time_interval_begin'].dt.month == 7)\n",
    "                      & (df['time_interval_begin'].dt.hour.isin([7, 14, 17])) & (\n",
    "                          df['time_interval_begin'].dt.minute == 58))].copy()\n",
    "\n",
    "    test_df['lagging5'] = test_df['lagging4']\n",
    "    test_df['lagging4'] = test_df['lagging3']\n",
    "    test_df['lagging3'] = test_df['lagging2']\n",
    "    test_df['lagging2'] = test_df['lagging1']\n",
    "    test_df['lagging1'] = test_df['travel_time']\n",
    "\n",
    "    with open(file1, 'w'):\n",
    "        pass\n",
    "    with open(file2, 'w'):\n",
    "        pass\n",
    "    with open(file3, 'w'):\n",
    "        pass\n",
    "    with open(file4, 'w'):\n",
    "        pass\n",
    "\n",
    "    for i in range(30):\n",
    "        test_X = test_df[train_feature]\n",
    "        y_prediction = regressor.predict(test_X.values)\n",
    "\n",
    "        test_df['lagging5'] = test_df['lagging4']\n",
    "        test_df['lagging4'] = test_df['lagging3']\n",
    "        test_df['lagging3'] = test_df['lagging2']\n",
    "        test_df['lagging2'] = test_df['lagging1']\n",
    "        test_df['lagging1'] = y_prediction\n",
    "\n",
    "        test_df['predicted'] = np.expm1(y_prediction)\n",
    "        test_df['time_interval_begin'] = test_df['time_interval_begin'] + pd.DateOffset(minutes=2)\n",
    "        test_df['time_interval'] = test_df['time_interval_begin'].map(\n",
    "            lambda x: '[' + str(x) + ',' + str(x + pd.DateOffset(minutes=2)) + ')')\n",
    "        test_df.time_interval = test_df.time_interval.astype(object)\n",
    "        if i < 7:\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file1, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        elif (7 <= i) and (i < 14):\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file2, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        elif (14 <= i) and (i < 22):\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file3, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')\n",
    "        else:\n",
    "            test_df[['link_ID', 'date', 'time_interval', 'predicted']].to_csv(file4, mode='a', header=False,\n",
    "                                                                              index=False,\n",
    "                                                                              sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.99822675145421"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.80786214674780721+0.8240044555484739+0.79502990524713169+0.79436080458845282+0.77696943932234408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20035464970915806"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- 3.99822675145421/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
